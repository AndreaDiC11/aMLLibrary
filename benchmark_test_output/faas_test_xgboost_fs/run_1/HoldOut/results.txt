Evaluate experiments (sequentially)
<--
Printing results for run 0
Best result for Technique.XGBOOST - Configuration is ['min_child_weight_1', 'gamma_0', 'n_estimators_150', 'learning_rate_0.1', 'max_depth_1'] - (Training MAPE is 0.008119 - HP Selection MAPE is 0.101676) - Validation MAPE is 0.052877
Overall best result is Technique.XGBOOST ['min_child_weight_1', 'gamma_0', 'n_estimators_150', 'learning_rate_0.1', 'max_depth_1']
Metrics for best result:
-->
MAPE: (Training 0.008119 - HP Selection 0.101676) - Validation 0.052877
RMSE: (Training 0.010751 - HP Selection 0.173231) - Validation 0.175412
R^2 : (Training 0.999899 - HP Selection 0.963131) - Validation 0.957788
<--
Building the final regressors
Validation metrics on full dataset for Technique.XGBOOST:
-->
MAPE: 0.072961
RMSE: 0.118421
R^2 : 0.989439
<--
Built the final regressors
Best model:
-->
XGBoost weights: {
    0.683 warm_service_time
    0.28 Lambda
    0.037 cold_service_time
}
<--
