[DataPreparation]
seed = 1234
data_preparation_module = tf_deepspeech
target_column = 28
input_name = tf_deepspeech
sparkdl_run =
test_size = 0.2
split = split2
image_nums_train_data = [250, 1000]
image_nums_test_data = [750]
case = case1
core_nums_train_data = [6, 10, 14, 18, 24, 28, 32, 36, 40, 44]
core_nums_test_data = [8, 12, 16, 22, 26, 30, 34, 38, 42]
use_spark_info = False
normalize_data = True
percentage_large_error = 30.0
remove_fractional = True

[tf_deepspeech]
#Tuple or list of Tuple:('Feature_Name',max_train(included), min_test(not included!))
extrapolate =
#('batch size(13)',8,8)
#('Real Iterations Number(15)',320,320)
#[('Network depth(11)',400,1600),('batch size(13)',8,32)]
include_inverse = True
features =  ['Real Iterations Number(15)',
            'batch size(13)',
            'Network depth(11)',
            'GPU number(9)',
            'CPU threads(19)',
            'GFlops(7)',
            'disk speed(8)',
            'Iterations Fraction(14)',
            'Computed Iterations Number(16)',
            'epochs(18)',
            'overall execution time(30)']

#Prova feature extrapolation GPU Number
generate_features =
#Feature selezionate fino all'8 grado
                #   [
                #   'batch size(13)-batch size(13)-Inv_GFlops(7)-Inv_GFlops(7)-Inv_GPU number(9)-Network depth(11)-Real Iterations Number(15)-Real Iterations Number(15)',
                #   'batch size(13)-batch size(13)-Inv_GFlops(7)-Inv_GPU number(9)-Inv_GPU number(9)-Network depth(11)-Real Iterations Number(15)-Real Iterations Number(15)',
                #   'batch size(13)-Inv_GFlops(7)-Inv_GFlops(7)-Inv_GFlops(7)-Inv_GPU number(9)-Real Iterations Number(15)-Real Iterations Number(15)',
                #   'batch size(13)-Inv_GFlops(7)-Inv_GFlops(7)-Inv_GPU number(9)-Network depth(11)-Real Iterations Number(15)-Real Iterations Number(15)'
                #   ]
# ['Real Iterations Number(15)',
# 'Real Iterations Number(15)-batch size(13)-disk speed(8)',
# 'Real Iterations Number(15)-GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-GPU number(9)-Inv_CPU threads(19)-Inv_GFlops(7)-Inv_disk speed(8)',
# 'batch size(13)-Inv_GPU number(9)-Inv_CPU threads(19)-GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-GPU number(9)-Inv_CPU threads(19)-GFlops(7)-disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-GPU number(9)-CPU threads(19)-Inv_GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-Inv_GPU number(9)-Inv_GFlops(7)',
# 'Real Iterations Number(15)-batch size(13)-Inv_GPU number(9)-Inv_CPU threads(19)-GFlops(7)-Inv_disk speed(8)',
# 'batch size(13)-Inv_GFlops(7)-disk speed(8)-Real Iterations Number(15)-Inv_GPU number(9)-Inv_CPU threads(19)-GFlops(7)-disk speed(8)',
# 'GPU number(9)-GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-GPU number(9)-CPU threads(19)-GFlops(7)-disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-Inv_GPU number(9)-GFlops(7)-disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-Inv_GPU number(9)-CPU threads(19)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-GPU number(9)-Inv_CPU threads(19)-Inv_GFlops(7)-disk speed(8)',
# 'batch size(13)-GPU number(9)-Inv_CPU threads(19)-Inv_GFlops(7)-disk speed(8)',
# 'batch size(13)-Inv_CPU threads(19)-GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-Inv_GPU number(9)-Inv_CPU threads(19)-Inv_GFlops(7)-disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-Inv_CPU threads(19)-GFlops(7)-disk speed(8)-Inv_GPU number(9)-Inv_GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-batch size(13)-GPU number(9)-Inv_GFlops(7)-disk speed(8)',
# 'batch size(13)-Inv_GPU number(9)-CPU threads(19)-GFlops(7)-Inv_disk speed(8)',
# 'GPU number(9)-Inv_CPU threads(19)-GFlops(7)-Inv_disk speed(8)',
# 'Real Iterations Number(15)-GPU number(9)-Inv_CPU threads(19)-Inv_GFlops(7)-disk speed(8)',
# 'CPU threads(19)-Inv_GFlops(7)-Inv_disk speed(8)',
# 'Inv_GPU number(9)-Inv_CPU threads(19)-GFlops(7)-disk speed(8)',
# 'batch size(13)-GPU number(9)-disk speed(8)']

[DebugLevel]
debug = True # This is for printing the logs. If debug is true, we also print the logs in the DEBUG level. Otherwise, only the logs in INFO level is printed.

[FeatureExtender]
n_terms = [2]
#[2,3]
[FeatureSelector]
method = 'xgb'
# 'ada' or 'sfs' or 'nosfs' or 'xgb'

sfs_n_jobs = -1
grid_n_jobs = -1
select_features_vif = False
select_features_sfs = True
min_features = 1
max_features = 6
is_floating = True
fold_num = 5
regressor_name = ridge
# da 0.1 ad 1, altrimenti esploro sopra
ridge = [0.001,0.01,0.1,1,100,1000]
lasso = [0.1,0.5]

#max_features_dt = [sqrt]
#max_depth_dt = [7]
#min_samples_leaf_dt = [4]
#min_samples_split_dt = [5]

max_features_dt = [auto,sqrt]
max_depth_dt = [10, 20, 30]
min_samples_leaf_dt = [1, 2, 4]
min_samples_split_dt = [2, 5, 10]

n_estimators = [20, 40, 60, 80]
max_features_rf = [auto,sqrt]
max_depth_rf = [10, 20, 30]
min_samples_leaf_rf = [5, 6, 7]
min_samples_split_rf = [2, 5, 10]
bootstrap = [True, False]
kernel = [rbf]
c = [10]
epsilon = [0.1]
degree = [3]
gamma = [0.01]
n_neighbors = [5,10]

[AdaBoost]
n_estimators = [200]
# [50, 100]
learning_rate = [0.025]
# [0.01, 0.05, 0.1, 0.3, 1]
loss = ['linear', 'square', 'exponential']
# ['linear', 'square', 'exponential']

[XGBoost]
xgb_n_jobs = -1
booster= 'gbtree'
#Specify which booster to use: gbtree[default], gblinear or dart

learning_rate = [0.2]
# [0.1]
# questo Ã¨ il learning rate per prove oppure da 0.05 a 0.3
min_child_weight = [2]
max_depth = [5]
# [3,6,10]
gamma = [0]
reg_lambda = [0.01]
#[1] default
reg_alpha = [0]
subsample = [1]


